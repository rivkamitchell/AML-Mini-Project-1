import numpy as np 

class logRegression:
	def __init__(self, learnrate, iterations): 						# A simple constructor!
		self.learnrate = learnrate
		self.iterations = iterations


	def sigmoid(self, w):										            	#Making the sigmoid function for Log Regression
		return (1 / (1 + np.exp(-w)))

	def lossFunction(self, X, y):					
		return -(np.dot(y.T * log(h), (1-y)*log(1-h)))			#This is the loss function we saw in class

	def fit(self, X, y):
		weights = np.zeros(X.Shape[1])							      	#initialize weights as a zero array
                                                    
		for i in range(self.iterations):
			z = np.dot(weights.T, X)								          #take the dot product of input array X and our weight vector transposed
			h = self.__sigmoid(z)								            	#run our weighted sum through the sigmoid function

			grad = np.dot(X.T * y, (y - h)) / y.shape[0]			#The gradient we saw in class

			weights -= learnrate * grad         				    	#Modeling the update rule from lecture 4!

	def pred_prob(self, X):									          		#runs X through sigmoid to get a probability
		return self.__sigmoid(np.dot(weights.T, X))

	def predictor3000(self, X, threshold):			  				#if X has probability greater than threshold we get a boolean true babyyyyyy
		return self.pred_prob(X) >= threshold
