import numpy as np
import pandas as pd
import requests as recs
import seaborn as sb

class LDA: 
	def __init__(self, X, n, m):
		self.data = X
		self.features = X[:,:n-1]
		self.quality = X[:,n-1]
		(self.pos_probability, self.neg_probability) = self.distributions()
		(self.positive, self.negative, self.pos_count, self.neg_count, self.total) = self.separate()

		self.cov = self.covariance()
		self.cov_inv = np.linalg.inv(self.cov)
		#(self.positive, self.negative, self.pos_count, self.neg_count, self.count) = self.separate()
	def distributions(self):
		positive_mean = np.mean(self.data, axis = 0)[len(self.data[0]) - 1]
		negative_mean = 1 - positive_mean
		return (positive_mean, negative_mean)

	def separate(self):
    	
		positive_class = []
		negative_class = []
		num_neg = 0
		total = 0
		for x in self.data:
    		
			total += 1
			if x[len(x)-1] == 0: 
				num_neg += 1
				negative_class.append(x[:len(x)-1])
			else: positive_class.append(x[:len(x)-1])
		return (np.array(positive_class), np.array(negative_class), total - num_neg, num_neg, total)

	
	def covariance(self):
		X_pos = self.positive
		X_neg = self.negative

		ones_neg=np.ones(X_neg.shape[0]*X_neg.shape[1]).reshape(X_neg.shape[0], X_neg.shape[1]) #using matrix mult, not optimal..
		ones_neg[:,:] = np.mean(X_neg, axis = 0)

		ones_pos=np.ones(X_pos.shape[0]*X_pos.shape[1]).reshape(X_pos.shape[0], X_pos.shape[1]) 
		ones_pos[:,:]= np.mean(X_pos, axis = 0)
		
		X_0 = X_neg - ones_neg
		X_1 = X_pos - ones_pos

		neg_sum = np.zeros((len(X_neg[0]),len(X_neg[0])))
		for x in X_0: 
			array_x = []
			for y in x: array_x.append(y*x)
			neg_sum += array_x
		
	
		for x in X_1: 
			array_x = []			
			
			for y in x: 
				array_x.append(y*x)
    		
			neg_sum += array_x	
		
		covariance = neg_sum/(len(X_neg) + len(X_pos) - 2)
		return covariance

	def fit(self): return (self.covariance, np.mean(self.positive), np.mean(self.negative))

	def predict(self, x):
    	
		log = np.log(self.pos_probability/self.neg_probability)
		
		pos_mean = np.mean(self.positive, axis = 0)
		neg_mean = np.mean(self.negative, axis = 0)

		pos_part = -(1/2)*np.matmul(np.transpose(pos_mean),np.matmul(self.cov_inv, pos_mean))
		neg_part = (1/2)*np.matmul(np.transpose(neg_mean), np.matmul(self.cov_inv, neg_mean))

		log_ratio = log + pos_part + neg_part + np.matmul(np.transpose(x),np.matmul(self.cov_inv, (pos_mean - neg_mean)))

		if log_ratio > 0: return 1
		return 0
	
	def accuracy(self): 
		tp = 0
		tn = 0
		fp = 0
		fn = 0
		for x in self.data:
    		
			if x[len(x) - 1] == 1 and self.predict(x[:len(x) - 1]) == 1: tp += 1
			elif x[len(x) - 1] == 0 and self.predict(x[:len(x) - 1]) == 0: tn += 1
			elif x[len(x) - 1] == 0 and self.predict(x[:len(x) - 1]) == 1: fp += 1
			elif x[len(x) - 1] == 1 and self.predict(x[:len(x) - 1]) == 0: fn += 1
		
		return (tp + tn)/(tp + fp + fn + tn)

url_1 ="https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
wine=pd.read_csv(url_1, sep=";")
wine['quality'] = wine['quality'].apply(lambda t: 1 if t > 5 else 0)
wine = np.array(wine)

model = LDA(wine, 12, 1598)
print(model.accuracy())
