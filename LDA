import numpy as np
import pandas as pd
import requests as recs
import seaborn as sb

class LDA: 
	def __init__(self, X):
		(self.num_exp, self.num_features)=X.shape
		self.data = X #full Data
		self.features = X[:,:self.num_features-1] #just the features, not the result
		self.quality = X[:,self.num_features-1] #just the result
		(self.positive, self.negative, self.pos_count, self.neg_count, self.total) = self.separate()
		(self.pos_prob, self.neg_prob)=(self.pos_count/self.total, self.neg_count/self.total)
		self.cov = self.covariance()#covariance matrix using features
		self.cov_inv = np.linalg.inv(self.cov)

		self.validation=X
		#(self.positive, self.negative, self.pos_count, self.neg_count, self.count) = self.separate()

	def separate(self):
		positive_class = []
		negative_class = []
		num_neg = 0
		total = 0
		for x in self.data:
			total += 1
			if x[len(x)-1] == 0: 
				num_neg += 1
				negative_class.append(x[:len(x)-1])
			else: positive_class.append(x[:len(x)-1])
		return (np.array(positive_class), np.array(negative_class), total - num_neg, num_neg, total)


	def covariance(self):
		X_pos = self.positive
		X_neg = self.negative

		ones_neg=np.ones(X_neg.shape[0]*X_neg.shape[1]).reshape(X_neg.shape[0], X_neg.shape[1]) #using matrix mult, not optimal..
		ones_neg[:,:] = np.mean(X_neg, axis = 0)

		ones_pos=np.ones(X_pos.shape[0]*X_pos.shape[1]).reshape(X_pos.shape[0], X_pos.shape[1]) 
		ones_pos[:,:]= np.mean(X_pos, axis = 0)
		
		X_0 = X_neg - ones_neg
		X_1 = X_pos - ones_pos

		neg_sum = np.zeros((len(X_neg[0]),len(X_neg[0])))
		for x in X_0: 
			array_x = []
			for y in x: array_x.append(y*x)
			neg_sum += array_x


		for x in X_1: 
			array_x = []
			for y in x: 
				array_x.append(y*x)
			neg_sum += array_x	
		
		covariance = neg_sum/(len(X_neg) + len(X_pos) - 2)
		return covariance

	def fit(self): return (self.covariance, np.mean(self.positive), np.mean(self.negative))

	def predict(self, x):

		log = np.log(self.pos_prob/self.neg_prob)
		
		pos_mean = np.mean(self.positive, axis = 0)
		neg_mean = np.mean(self.negative, axis = 0)

		pos_part = -(1/2)*np.matmul(np.transpose(pos_mean),np.matmul(self.cov_inv, pos_mean))
		neg_part = (1/2)*np.matmul(np.transpose(neg_mean), np.matmul(self.cov_inv, neg_mean))
		
		log_ratio = log + pos_part + neg_part + np.matmul(np.transpose(x),np.matmul(self.cov_inv, (pos_mean - neg_mean)))

		if log_ratio > 0: return 1
		return 0
	
	def accuracy(self, vali): 
		correct=0
		wrong=0
		self.validation=vali
		for x in vali:
			if x[-1]==self.predict(x[:len(x)-1]) : correct+=1
			else: wrong+=1


		return (correct)/(correct+wrong)

	def k_fold_crossing(self, k):
		#assuming k divides the number of experiments. But should b fine otherwise
		size=int(self.num_exp/k)
		acc=0
		for i in range(k):
			temp1=i*size
			temp2=(i+1)*size
			train=np.vstack((self.data[:temp1], self.data[temp2:]))
			vali=self.data[temp1:temp2]
			model=LDA(train)
			acc+=model.accuracy(vali)
		return (acc/k)

