import numpy as np
import preprocessing

(wine, cancer) = preprocessing.data()

wine_d = wine[:, :len(wine[0])-1]
wine_labels = wine[:,len(wine[0])-1]
cancer_d = cancer[: , :len(cancer[0])-1]
cancer_labels = cancer[:,len(cancer[0])-1]

def getGoodData(X):
	Y = np.zeros((X.shape[0], 1))
	for y in Y:
		y[0] =1
	Z = np.append(X, Y, axis =1)
	return Z
def gimmewinedata():
	wine_data = getGoodData(wine_d)
	return wine_data
def gimmewinelabels():
	return wine_labels
def gimmecancerdata():
	cancer_data = getGoodData(cancer_d)
	return cancer_data
def gimmecancerlabels():
	return cancer_labels

X = gimmewinedata()
y = gimmewinelabels()





class logRegression:
	def __init__(self, iterations): 						# A simple constructor!

		self.iterations = iterations


	def sigmoid(self, w):
			return (1 / (1 + np.exp(-w)))					        #Making the sigmoid function for Log Regression

	def lossFunction(self, X, y):					
		return -(np.dot(y.T * log(h), (1-y)*log(1-h)))				#This is the loss function we saw in class

	def fit(self, X, y):
		self.weights = np.zeros(X.shape[1])								#initialize weights as a zero array

		for i in range(self.iterations):
			z = np.dot(X, self.weights.T)
											#take the dot product of input array X and our weight vector
			h = self.sigmoid(z)									#run our weighted sum through the sigmoid function
			
			grad = np.dot(X.T, (y - h))						#The gradient we saw in class
			
			if i<(self.iterations/4):
				learnrate = 1/2
			elif i<(self.iterations/2):
				learnrate = .01
			else:
				learnrate = 1/(i+1)

			self.weights += learnrate * grad
						        					#Modeling the update rule from lecture 4!

	def pred_prob(self, x):											#runs X through sigmoid to get a probability
		return self.sigmoid(np.dot(x, self.weights))

	def predictor3000(self, x, threshold):							#if X has probability greater than threshold we get a boolean true babyyyyyy
		return self.pred_prob(x) >= threshold

	def accuracy(self, X): 
		tp = 0
		tn = 0
		fp = 0
		fn = 0
		for i in range(len(y)):
			if y[i] == 1 and self.predictor3000(X[i], 0.5) == True: tp += 1
			elif y[i] == 0 and self.predictor3000(X[i], 0.5) == False: tn += 1
			elif y[i] == 0 and self.predictor3000(X[i], 0.5) == True: fp += 1
			elif y[i] == 1 and self.predictor3000(X[i], 0.5) == False: fn += 1
		
		return (tp + tn)/(tp + fp + fn + tn)



model = logRegression(iterations = 3000)
model.fit(X,y)

print(model.accuracy(X))
print(model.weights)
print(model.predictor3000(X[0], .5))
