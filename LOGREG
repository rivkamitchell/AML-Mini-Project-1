import numpy as np
import Preprocessing

(wine, cancer) = Preprocessing.data()

wine_data = wine[:, :len(wine[0])-1]                    #Getting data and labels from preprocessing
wine_labels = wine[:,len(wine[0])-1]

cancer_data = cancer[: , :len(cancer[0])-1]
cancer_labels = cancer[:,len(cancer[0])-1]


def addFeatureCol(X, numCols):							#A function that adds irrelevant feature
	Y = np.zeros((X.shape[0], 1))						#columns to the data array.
	for y in Y:
		y[0] =1
	for i in range(numCols):
		X = np.append(X, Y, axis =1)
	return X

class logRegression:
	def __init__(self, iterations): 								#A simple constructor!
		self.iterations = iterations

	def sigmoid(self, w):
			return (1 / (1 + np.exp(-w)))					        #Making the sigmoid function for Log Regression

	def lossFunction(self, X, y):					
		return -(np.dot(y.T * log(h), (1-y)*log(1-h)))				#This is the loss function we saw in class

	def fit(self, X, y):
		self.weights = np.zeros(X.shape[1])							#initialize weights as a zero array

		for i in range(self.iterations):
			z = np.dot(X, self.weights.T)							#take the dot product of input array X and our weight vector
											
			h = self.sigmoid(z)										#run our weighted sum through the sigmoid function
			
			grad = np.dot(X.T, (y - h))								#The gradient for Cross Entropy Loss we found in class				
	
			learnrate = 3/(2*i+1)

			self.weights += learnrate * grad 						#The update rule from lecture 4
						        					
	def pred_prob(self, x):											#runs a x, a row of X, through sigmoid to get a probability for that row
		return self.sigmoid(np.dot(x, self.weights))

	def predictor3000(self, x, threshold):							#if x has probability greater than threshold we get a boolean true babyyyyyy
		return self.pred_prob(x) >= threshold

	def accuracy(self, X): 											#a function that checks the accuracy of our model
		tp = 0
		tn = 0
		fp = 0
		fn = 0
		for i in range(len(y)):
			if y[i] == 1 and self.predictor3000(X[i], 0.5) == True: tp += 1
			elif y[i] == 0 and self.predictor3000(X[i], 0.5) == False: tn += 1
			elif y[i] == 0 and self.predictor3000(X[i], 0.5) == True: fp += 1
			elif y[i] == 1 and self.predictor3000(X[i], 0.5) == False: fn += 1
		return (tp + tn)/(tp + fp + fn + tn)


X = addFeatureCol(wine_data, 3)		
y = wine_labels

model = logRegression(iterations = 12000)
model.fit(X,y)
print(model.accuracy(X))

print(model.weights)
print(model.predictor3000(X[0], .5))
