import numpy as np:

class LDAmodel:

	def __init__(self, X, n, m):
		#X is an nx(m+1) matrix with (feature matrix | results) AS np object. n rows $m+1 columns so len(X)=n
		self.data=X
		self.feature= X[:,:m]

		self.result=X[:,m+1]

		self.covariance#we'll have to call a function on this
		self.positive  #function outputting the positive things
		self.negative   #function outputting....
		self.means # vector of total means of features
		self.means_positive #vector of positive means
		self.means_negative  #vectore of negative


		(self.positive, self.negative, self.neg_probability, self.pos_probability, self.total)=self.separate(X)


		#are means really necessary?
		(self.means, self.means_positive, self.means_negative)=(self.mean(self.feature), self.mean(self.positive), self.mean(self.negative))

		(self.covariance, self.positive....... )=self.fit()

		self.cov_inverse=np.linalg.inv(covariance)

	def separate(self):                          # X: nx(m+1) matrix
        positive_class = []
        negative_class = []
        number_of_neg=0
        total=0
        for x in self.data: 
        	total+=1
            if x[len(x)] == 0:                                              # Note this is possible with our data sets as we always have feature vectors of a certain length
                negative_class.append(x[:len(x) - 1])
                number_of_neg+=1
            else:
                positive_class.append(x[:len(x) - 1])

        return (np.array(positive_class), np.array(negative_class), number_of_neg/total, (total-number_of_neg)/total, total )


    def mean(self, arr): #numpy array object, returns the means 
    	return np.mean(arr, axis=0)


        # Determine covariance
    def covariance(self, X):
        X_neg = self.negative
        X_pos = self.positive

        ones_neg=np.ones(X_neg.shape[0]) #using matrix mult, not optimal..
        ones_pos=np.ones(X_pos.shape[0])

        X_0 = X_neg - np.mean(X_neg, axis = 0)@ones_neg #should be the right dimension: to check!
        X_1 = X_pos - np.mean(X_pos, axis = 0)@ones_pos


        #X time X transpose should be nxn where n is the number of feature
        covariance = (X_0@np.transpose(X_0) + X_1@np.transpose(X_1))/(len(X_neg) + len(X_pos) - 2)
        
        return covariance



	def fit(self): #this function will output the covariance, means and
		

		return(self. covariance, self.means_positive, self.means_negative)

	def predict(self, x): #here x is a length m vector or list

		inv_covariance=np.linalg.inv(self.covariance) #or =cov_inverse
		log=np.log(self.pos_probability/self.neg_probability)
		pos_part=  -(1/2)*np.transpose(self.means_positive)@inv_covariance@means_positive
		neg_part= -(1/2)*np.transpos(self.means_negative)@inv_covariance@means_negative

		log_ratio=log +pose_part+neg_part + np.transpos(x)@inv_covariance@(means_positive-means_negative)

		if log_ratio>0:
			return 1

		return 0



